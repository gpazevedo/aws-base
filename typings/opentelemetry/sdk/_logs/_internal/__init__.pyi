"""
This type stub file was generated by pyright.
"""

import abc
import atexit
import base64
import concurrent.futures
import json
import logging
import threading
import traceback
import warnings
from os import environ
from threading import Lock
from time import time_ns
from typing import Any, Callable, Tuple, Union, cast, overload
from typing_extensions import deprecated
from opentelemetry._logs import LogRecord as APILogRecord, Logger as APILogger, LoggerProvider as APILoggerProvider, NoOpLogger, SeverityNumber, get_logger, get_logger_provider
from opentelemetry.attributes import BoundedAttributes, _VALID_ANY_VALUE_TYPES
from opentelemetry.context import get_current
from opentelemetry.context.context import Context
from opentelemetry.sdk.environment_variables import OTEL_ATTRIBUTE_COUNT_LIMIT, OTEL_ATTRIBUTE_VALUE_LENGTH_LIMIT, OTEL_SDK_DISABLED
from opentelemetry.sdk.resources import Resource
from opentelemetry.sdk.util import ns_to_iso_str
from opentelemetry.sdk.util.instrumentation import InstrumentationScope
from opentelemetry.semconv._incubating.attributes import code_attributes
from opentelemetry.semconv.attributes import exception_attributes
from opentelemetry.trace import format_span_id, format_trace_id, get_current_span
from opentelemetry.trace.span import TraceFlags
from opentelemetry.util.types import AnyValue, _ExtendedAttributes

_logger = ...
_DEFAULT_OTEL_ATTRIBUTE_COUNT_LIMIT = ...
_ENV_VALUE_UNSET = ...
class BytesEncoder(json.JSONEncoder):
    def default(self, o): # -> str | Any:
        ...
    


class LogDroppedAttributesWarning(UserWarning):
    """Custom warning to indicate dropped log attributes due to limits.

    This class is used to filter and handle these specific warnings separately
    from other warnings, ensuring that they are only shown once without
    interfering with default user warnings.
    """
    ...


class LogDeprecatedInitWarning(UserWarning):
    """Custom warning to indicate that deprecated and soon to be deprecated Log classes was used.

    This class is used to filter and handle these specific warnings separately
    from other warnings, ensuring that they are only shown once without
    interfering with default user warnings.
    """
    ...


class LogLimits:
    """This class is based on a SpanLimits class in the Tracing module.

    This class represents the limits that should be enforced on recorded data such as events, links, attributes etc.

    This class does not enforce any limits itself. It only provides a way to read limits from env,
    default values and from user provided arguments.

    All limit arguments must be either a non-negative integer or ``None``.

    - All limit arguments are optional.
    - If a limit argument is not set, the class will try to read its value from the corresponding
      environment variable.
    - If the environment variable is not set, the default value, if any, will be used.

    Limit precedence:

    - If a model specific limit is set, it will be used.
    - Else if the corresponding global limit is set, it will be used.
    - Else if the model specific limit has a default value, the default value will be used.
    - Else if the global limit has a default value, the default value will be used.

    Args:
        max_attributes: Maximum number of attributes that can be added to a span, event, and link.
            Environment variable: ``OTEL_ATTRIBUTE_COUNT_LIMIT``
            Default: {_DEFAULT_OTEL_ATTRIBUTE_COUNT_LIMIT}
        max_attribute_length: Maximum length an attribute value can have. Values longer than
            the specified length will be truncated.
    """
    def __init__(self, max_attributes: int | None = ..., max_attribute_length: int | None = ...) -> None:
        ...
    
    def __repr__(self): # -> str:
        ...
    


class LogRecord(APILogRecord):
    """A LogRecord instance represents an event being logged.

    LogRecord instances are created and emitted via `Logger`
    every time something is logged. They contain all the information
    pertinent to the event being logged.
    """
    @overload
    def __init__(self, timestamp: int | None = ..., observed_timestamp: int | None = ..., context: Context | None = ..., severity_text: str | None = ..., severity_number: SeverityNumber | None = ..., body: AnyValue | None = ..., resource: Resource | None = ..., attributes: _ExtendedAttributes | None = ..., limits: LogLimits | None = ..., event_name: str | None = ...) -> None:
        ...
    
    @overload
    @deprecated("LogRecord init with `trace_id`, `span_id`, and/or `trace_flags` is deprecated since 1.35.0. Use `context` instead.")
    def __init__(self, timestamp: int | None = ..., observed_timestamp: int | None = ..., trace_id: int | None = ..., span_id: int | None = ..., trace_flags: TraceFlags | None = ..., severity_text: str | None = ..., severity_number: SeverityNumber | None = ..., body: AnyValue | None = ..., resource: Resource | None = ..., attributes: _ExtendedAttributes | None = ..., limits: LogLimits | None = ...) -> None:
        ...
    
    def __init__(self, timestamp: int | None = ..., observed_timestamp: int | None = ..., context: Context | None = ..., trace_id: int | None = ..., span_id: int | None = ..., trace_flags: TraceFlags | None = ..., severity_text: str | None = ..., severity_number: SeverityNumber | None = ..., body: AnyValue | None = ..., resource: Resource | None = ..., attributes: _ExtendedAttributes | None = ..., limits: LogLimits | None = ..., event_name: str | None = ...) -> None:
        ...
    
    def __eq__(self, other: object) -> bool:
        ...
    
    def to_json(self, indent: int | None = ...) -> str:
        ...
    
    @property
    def dropped_attributes(self) -> int:
        ...
    


class LogData:
    """Readable LogRecord data plus associated InstrumentationLibrary."""
    def __init__(self, log_record: LogRecord, instrumentation_scope: InstrumentationScope) -> None:
        ...
    


class LogRecordProcessor(abc.ABC):
    """Interface to hook the log record emitting action.

    Log processors can be registered directly using
    :func:`LoggerProvider.add_log_record_processor` and they are invoked
    in the same order as they were registered.
    """
    @abc.abstractmethod
    def on_emit(self, log_data: LogData): # -> None:
        """Emits the `LogData`"""
        ...
    
    @abc.abstractmethod
    def shutdown(self): # -> None:
        """Called when a :class:`opentelemetry.sdk._logs.Logger` is shutdown"""
        ...
    
    @abc.abstractmethod
    def force_flush(self, timeout_millis: int = ...): # -> None:
        """Export all the received logs to the configured Exporter that have not yet
        been exported.

        Args:
            timeout_millis: The maximum amount of time to wait for logs to be
                exported.

        Returns:
            False if the timeout is exceeded, True otherwise.
        """
        ...
    


class SynchronousMultiLogRecordProcessor(LogRecordProcessor):
    """Implementation of class:`LogRecordProcessor` that forwards all received
    events to a list of log processors sequentially.

    The underlying log processors are called in sequential order as they were
    added.
    """
    def __init__(self) -> None:
        ...
    
    def add_log_record_processor(self, log_record_processor: LogRecordProcessor) -> None:
        """Adds a Logprocessor to the list of log processors handled by this instance"""
        ...
    
    def on_emit(self, log_data: LogData) -> None:
        ...
    
    def shutdown(self) -> None:
        """Shutdown the log processors one by one"""
        ...
    
    def force_flush(self, timeout_millis: int = ...) -> bool:
        """Force flush the log processors one by one

        Args:
            timeout_millis: The maximum amount of time to wait for logs to be
                exported. If the first n log processors exceeded the timeout
                then remaining log processors will not be flushed.

        Returns:
            True if all the log processors flushes the logs within timeout,
            False otherwise.
        """
        ...
    


class ConcurrentMultiLogRecordProcessor(LogRecordProcessor):
    """Implementation of :class:`LogRecordProcessor` that forwards all received
    events to a list of log processors in parallel.

    Calls to the underlying log processors are forwarded in parallel by
    submitting them to a thread pool executor and waiting until each log
    processor finished its work.

    Args:
        max_workers: The number of threads managed by the thread pool executor
            and thus defining how many log processors can work in parallel.
    """
    def __init__(self, max_workers: int = ...) -> None:
        ...
    
    def add_log_record_processor(self, log_record_processor: LogRecordProcessor): # -> None:
        ...
    
    def on_emit(self, log_data: LogData): # -> None:
        ...
    
    def shutdown(self): # -> None:
        ...
    
    def force_flush(self, timeout_millis: int = ...) -> bool:
        """Force flush the log processors in parallel.

        Args:
            timeout_millis: The maximum amount of time to wait for logs to be
                exported.

        Returns:
            True if all the log processors flushes the logs within timeout,
            False otherwise.
        """
        ...
    


_RESERVED_ATTRS = ...
class LoggingHandler(logging.Handler):
    """A handler class which writes logging records, in OTLP format, to
    a network destination or file. Supports signals from the `logging` module.
    https://docs.python.org/3/library/logging.html
    """
    def __init__(self, level=..., logger_provider=...) -> None:
        ...
    
    def emit(self, record: logging.LogRecord) -> None:
        """
        Emit a record. Skip emitting if logger is NoOp.

        The record is translated to OTel format, and then sent across the pipeline.
        """
        ...
    
    def flush(self) -> None:
        """
        Flushes the logging output. Skip flushing if logging_provider has no force_flush method.
        """
        ...
    


class Logger(APILogger):
    def __init__(self, resource: Resource, multi_log_record_processor: Union[SynchronousMultiLogRecordProcessor, ConcurrentMultiLogRecordProcessor,], instrumentation_scope: InstrumentationScope) -> None:
        ...
    
    @property
    def resource(self): # -> Resource:
        ...
    
    @overload
    def emit(self, *, timestamp: int | None = ..., observed_timestamp: int | None = ..., context: Context | None = ..., severity_number: SeverityNumber | None = ..., severity_text: str | None = ..., body: AnyValue | None = ..., attributes: _ExtendedAttributes | None = ..., event_name: str | None = ...) -> None:
        ...
    
    @overload
    def emit(self, record: APILogRecord) -> None:
        ...
    
    def emit(self, record: APILogRecord | None = ..., *, timestamp: int | None = ..., observed_timestamp: int | None = ..., context: Context | None = ..., severity_text: str | None = ..., severity_number: SeverityNumber | None = ..., body: AnyValue | None = ..., attributes: _ExtendedAttributes | None = ..., event_name: str | None = ...): # -> None:
        """Emits the :class:`LogData` by associating :class:`LogRecord`
        and instrumentation info.
        """
        ...
    


class LoggerProvider(APILoggerProvider):
    def __init__(self, resource: Resource | None = ..., shutdown_on_exit: bool = ..., multi_log_record_processor: SynchronousMultiLogRecordProcessor | ConcurrentMultiLogRecordProcessor | None = ...) -> None:
        ...
    
    @property
    def resource(self): # -> Resource:
        ...
    
    def get_logger(self, name: str, version: str | None = ..., schema_url: str | None = ..., attributes: _ExtendedAttributes | None = ...) -> Logger:
        ...
    
    def add_log_record_processor(self, log_record_processor: LogRecordProcessor): # -> None:
        """Registers a new :class:`LogRecordProcessor` for this `LoggerProvider` instance.

        The log processors are invoked in the same order they are registered.
        """
        ...
    
    def shutdown(self): # -> None:
        """Shuts down the log processors."""
        ...
    
    def force_flush(self, timeout_millis: int = ...) -> bool:
        """Force flush the log processors.

        Args:
            timeout_millis: The maximum amount of time to wait for logs to be
                exported.

        Returns:
            True if all the log processors flushes the logs within timeout,
            False otherwise.
        """
        ...
    


_STD_TO_OTEL = ...
def std_to_otel(levelno: int) -> SeverityNumber:
    """
    Map python log levelno as defined in https://docs.python.org/3/library/logging.html#logging-levels
    to OTel log severity number as defined here: https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/logs/data-model.md#field-severitynumber
    """
    ...

